{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3df1ecb",
   "metadata": {},
   "source": [
    "# Train logistic regression model with all four plates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cc2637",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2882e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import random\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_recall_curve\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import parallel_backend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3faf66",
   "metadata": {},
   "source": [
    "## Find the root of the git repo on the host system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "224d1bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current working directory\n",
    "cwd = pathlib.Path.cwd()\n",
    "\n",
    "if (cwd / \".git\").is_dir():\n",
    "    root_dir = cwd\n",
    "\n",
    "else:\n",
    "    root_dir = None\n",
    "    for parent in cwd.parents:\n",
    "        if (parent / \".git\").is_dir():\n",
    "            root_dir = parent\n",
    "            break\n",
    "\n",
    "# Check if a Git root directory was found\n",
    "if root_dir is None:\n",
    "    raise FileNotFoundError(\"No Git root directory found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad2b1a4",
   "metadata": {},
   "source": [
    "## Load in all of the feature selected plates (post-filtering/single cell QC) and concat with the common features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d56a10",
   "metadata": {},
   "source": [
    "### Set paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25b74c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed\n",
    "rng = np.random.default_rng(0)\n",
    "\n",
    "# If the data (within the cell painting directory) is stored in a different location, add location here\n",
    "repo_dir = pathlib.Path(\n",
    "    root_dir / \"/media/18tbdrive/1.Github_Repositories/nf1_schwann_cell_painting_data\"\n",
    ")\n",
    "\n",
    "# Directory containing the feature selected parquet files (post-QC)\n",
    "data_dir = (\n",
    "    repo_dir / \"3.processing_features/data/single_cell_profiles/cleaned_sc_profiles\"\n",
    ")\n",
    "\n",
    "# Load in the four plate dataframes from data_dir\n",
    "plate3df = pd.read_parquet(data_dir / \"Plate_3_sc_feature_selected.parquet\")\n",
    "plate3pdf = pd.read_parquet(data_dir / \"Plate_3_prime_sc_feature_selected.parquet\")\n",
    "plate5df = pd.read_parquet(data_dir / \"Plate_5_sc_feature_selected.parquet\")\n",
    "plate6df = pd.read_parquet(data_dir / \"Plate_6_sc_feature_selected.parquet\")\n",
    "\n",
    "# Add Metadata_Institution column to each plate dataframe (except plate 6)\n",
    "plate3df[\"Metadata_Institution\"] = \"iNFixion\"\n",
    "plate3pdf[\"Metadata_Institution\"] = \"iNFixion\"\n",
    "plate5df[\"Metadata_Institution\"] = \"iNFixion\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dedcfb8",
   "metadata": {},
   "source": [
    "## Split and processes data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea541078",
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_column = \"Metadata_genotype\"\n",
    "\n",
    "\n",
    "def down_sample_by_genotype(_df):\n",
    "    \"\"\"\n",
    "    Return an equal number of cells from each genotype.\n",
    "    The number of cells in a genotype is the minimum number of cells from all genotypes.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    _df: Pandas Dataframe\n",
    "        The data to be downsampled by the gene_column column.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    The dataframe down-sampled by genotype.\n",
    "    \"\"\"\n",
    "\n",
    "    min_gene = _df[gene_column].value_counts().min()\n",
    "    return _df.groupby(gene_column, group_keys=False).apply(\n",
    "        lambda x: x.sample(n=min_gene, random_state=0)\n",
    "    )\n",
    "\n",
    "\n",
    "def process_plates(_df):\n",
    "    \"\"\"\n",
    "    Drop rows with nans from the single cell data and remove HET cells.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    _df: Pandas Dataframe\n",
    "        Uncleaned plate data with nans and HET cells to be removed. Contains the column \"Metadata_genotype\".\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    _df: Pandas Dataframe\n",
    "        Cleaned single cell data by removing nans and HET cells.\n",
    "    \"\"\"\n",
    "\n",
    "    _df.dropna(inplace=True)\n",
    "    _df = _df.loc[_df[gene_column] != \"HET\"]\n",
    "    return _df\n",
    "\n",
    "\n",
    "def shuffle_data(_X):\n",
    "    \"\"\"\n",
    "    Shuffle the columns of the input dataframe independently.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    _X: Pandas Dataframe\n",
    "        Input feature data for shuffling the columns.\n",
    "    \"\"\"\n",
    "\n",
    "    for column in _X.columns:\n",
    "        _X[column] = rng.permutation(_X[column])\n",
    "\n",
    "\n",
    "def create_splits(_wells, _plate):\n",
    "    \"\"\"\n",
    "    Create data splits for model training. The splits are rest (train and validation) and test.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    _wells: List(String)\n",
    "        The well names from which single cells will be used in the test set.\n",
    "\n",
    "    _plate: Pandas Dataframe\n",
    "        Single cell data from one of the plate's containing a \"Metadata_Well\" column.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Dataframes of the split single cell data.\n",
    "    \"\"\"\n",
    "\n",
    "    return (\n",
    "        _plate[~_plate[\"Metadata_Well\"].isin(_wells)],\n",
    "        _plate[_plate[\"Metadata_Well\"].isin(_wells)],\n",
    "    )\n",
    "\n",
    "\n",
    "def store_pre_evaluation_data(_X, _y, _metadata, _datasplit):\n",
    "    \"\"\"\n",
    "    Store model data to evaluate performance.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    _X: Pandas Dataframe\n",
    "        Feature dataframe from a given plate and data split.\n",
    "\n",
    "    _y: Numpy Array\n",
    "        A numerically-encoded label vector ordered according to _X.\n",
    "\n",
    "    _metadata: Pandas Dataframe\n",
    "        Plate name.\n",
    "\n",
    "    _datasplit: String\n",
    "        Data split name.\n",
    "    \"\"\"\n",
    "\n",
    "    eval_data[f\"probability_{probability_class}\"].extend(\n",
    "        logreg.predict_proba(_X)[:, 1].tolist()\n",
    "    )\n",
    "    eval_data[\"datasplit\"].extend([_datasplit] * _X.shape[0])\n",
    "    eval_data[\"predicted_genotype\"].extend(logreg.predict(_X).tolist())\n",
    "    eval_data[\"true_genotype\"].extend(_y.tolist())\n",
    "    for meta_col in _metadata.columns:\n",
    "        eval_data[meta_col].extend(_metadata[meta_col].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91306ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plate3df = process_plates(plate3df)\n",
    "p3_wells = [\"C11\", \"E11\", \"C3\", \"F3\"]\n",
    "rest3df, test3df = create_splits(p3_wells, plate3df)\n",
    "rest3df, test3df = down_sample_by_genotype(rest3df), down_sample_by_genotype(test3df)\n",
    "\n",
    "plate3pdf = process_plates(plate3pdf)\n",
    "p3p_wells = [\"F11\", \"G11\", \"C3\", \"F3\"]\n",
    "rest3pdf, test3pdf = create_splits(p3p_wells, plate3pdf)\n",
    "rest3pdf, test3pdf = down_sample_by_genotype(rest3pdf), down_sample_by_genotype(\n",
    "    test3pdf\n",
    ")\n",
    "\n",
    "plate5df = process_plates(plate5df)\n",
    "p5_wells = [\"C9\", \"E11\", \"E3\", \"G3\"]\n",
    "rest5df, test5df = create_splits(p5_wells, plate5df)\n",
    "rest5df, test5df = down_sample_by_genotype(rest5df), down_sample_by_genotype(test5df)\n",
    "\n",
    "plate6df = process_plates(plate6df)\n",
    "p6_wells = [\"E9\", \"B10\", \"D11\", \"D4\"]\n",
    "rest6df, test6df = create_splits(p6_wells, plate6df)\n",
    "rest6df, test6df = down_sample_by_genotype(rest6df), down_sample_by_genotype(test6df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0c672da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns common to all plates\n",
    "plate_cols = list(\n",
    "    set(plate5df.columns)\n",
    "    & set(plate3df.columns)\n",
    "    & set(plate3pdf.columns)\n",
    "    & set(plate6df.columns)\n",
    ")\n",
    "\n",
    "# Set up combined rest and test dataframes\n",
    "restdf = pd.concat(\n",
    "    [\n",
    "        rest3df[plate_cols],\n",
    "        rest3pdf[plate_cols],\n",
    "        rest5df[plate_cols],\n",
    "        rest6df[plate_cols],\n",
    "    ],\n",
    "    ignore_index=True,\n",
    ").reset_index(drop=True)\n",
    "\n",
    "testdf = pd.concat(\n",
    "    [\n",
    "        test3df[plate_cols],\n",
    "        test3pdf[plate_cols],\n",
    "        test5df[plate_cols],\n",
    "        test6df[plate_cols],\n",
    "    ],\n",
    "    ignore_index=True,\n",
    ").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef23d515",
   "metadata": {},
   "source": [
    "## Encode genotypes and set up X,y data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bbd224a",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_cols = testdf.filter(like=\"Metadata\").columns\n",
    "feat_cols = testdf.drop(columns=meta_cols).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0a36334",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "\n",
    "y = le.fit_transform(restdf[\"Metadata_genotype\"])\n",
    "X = restdf.drop(columns=meta_cols)\n",
    "\n",
    "y_test = le.fit_transform(testdf[\"Metadata_genotype\"])\n",
    "X_test = testdf.drop(columns=meta_cols)\n",
    "\n",
    "# Class for saving probabilities\n",
    "probability_class = le.inverse_transform([1])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ad04041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null    9646\n",
      "WT      9646\n",
      "Name: Metadata_genotype, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(restdf[\"Metadata_genotype\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a0c2ab",
   "metadata": {},
   "source": [
    "## Train logistic regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54418d36",
   "metadata": {},
   "source": [
    "### Specify parameters for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a69b5d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = pathlib.Path(\"./models\")\n",
    "model_dir.mkdir(parents=True, exist_ok=True)\n",
    "final_model_path = pathlib.Path(f\"{model_dir}/best_final_logreg_model.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "831e2ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_params = {\n",
    "    \"max_iter\": 250,\n",
    "    \"random_state\": 0,\n",
    "    \"n_jobs\": -1,\n",
    "    \"penalty\": \"l2\",\n",
    "}\n",
    "\n",
    "# Random sampling range of hyperparameter\n",
    "param_ranges = {\"C\": (0, 200)}\n",
    "\n",
    "# Number of iteration to optimize hyperparameters\n",
    "rand_iter = 500\n",
    "\n",
    "# Best accuracy\n",
    "best_acc = 0\n",
    "\n",
    "# Initial accuracy\n",
    "acc = 0\n",
    "\n",
    "# Number of folds\n",
    "n_splits = 8\n",
    "\n",
    "# Generate hyperparameter samples\n",
    "random_params = {\n",
    "    i: {key: random.uniform(*param_ranges[key]) for key in param_ranges}\n",
    "    for i in range(rand_iter)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c5595b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model already exists at models/best_final_logreg_model.joblib, skipping hyperparameter search.\n"
     ]
    }
   ],
   "source": [
    "# Check if best model already exists\n",
    "model_path = pathlib.Path(\"./models/best_final_logreg_model.joblib\")\n",
    "if model_path.exists():\n",
    "    print(f\"Model already exists at {model_path}, skipping hyperparameter search.\")\n",
    "else:\n",
    "    # Store model results for evaluation\n",
    "    eval_data = defaultdict(list)\n",
    "\n",
    "    best_acc = 0  # Initialize best accuracy\n",
    "    best_hp = None  # Track best hyperparameters\n",
    "\n",
    "    # Iterate through hyperparameters\n",
    "    for idx, rparams in random_params.items():\n",
    "\n",
    "        skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=0)\n",
    "\n",
    "        # Combine parameters in current search with logistic regression parameters\n",
    "        comb_params = logreg_params | rparams\n",
    "        acc = 0  # Accuracy accumulator for this parameter set\n",
    "\n",
    "        # Loop through the folds\n",
    "        for fold, (train_index, val_index) in enumerate(skf.split(X, y)):\n",
    "\n",
    "            X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "            y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "            X_val_shuf = X_val.copy()\n",
    "            shuffle_data(X_val_shuf)\n",
    "\n",
    "            # Prevent the convergence warning in sklearn\n",
    "            with parallel_backend(\"multiprocessing\"):\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.filterwarnings(\n",
    "                        \"ignore\", category=ConvergenceWarning, module=\"sklearn\"\n",
    "                    )\n",
    "                    logreg = LogisticRegression(**comb_params)\n",
    "                    logreg.fit(X_train, y_train)\n",
    "\n",
    "            # Cumulative accuracy for all folds\n",
    "            preds = logreg.predict(X_val)\n",
    "            preds_shuf = logreg.predict(X_val_shuf)\n",
    "            acc += accuracy_score(y_val, preds)\n",
    "\n",
    "            store_pre_evaluation_data(\n",
    "                X_val, y_val, restdf.iloc[val_index][meta_cols], \"val\"\n",
    "            )\n",
    "            store_pre_evaluation_data(\n",
    "                X_val_shuf, y_val, restdf.iloc[val_index][meta_cols], \"shuffled_val\"\n",
    "            )\n",
    "\n",
    "        # Average accuracy for the folds\n",
    "        acc = acc / n_splits\n",
    "\n",
    "        # Store the data with the best performance\n",
    "        if acc > best_acc:\n",
    "            best_hparam = eval_data.copy()\n",
    "            best_acc = acc\n",
    "            best_hp = rparams\n",
    "\n",
    "    print(f\"Best average validation accuracy = {best_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f3e2f9",
   "metadata": {},
   "source": [
    "Due to error in the code, the cells were not run in order. We want to make sure they are ran in order, but not rerun the model which takes 7 hours to train. \n",
    "\n",
    "As reference, the best average validation accuracy for the hyperparameter search is **0.9173886180912052**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826c1f1e",
   "metadata": {},
   "source": [
    "## Train final optimized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f0f7b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final model already exists at models/best_final_logreg_model.joblib, skipping training.\n"
     ]
    }
   ],
   "source": [
    "# Save path\n",
    "model_dir = pathlib.Path(\"./models\")\n",
    "model_dir.mkdir(parents=True, exist_ok=True)\n",
    "model_path = model_dir / \"best_final_logreg_model.joblib\"\n",
    "\n",
    "# Only train if the model doesn't already exist\n",
    "if model_path.exists():\n",
    "    print(f\"Final model already exists at {model_path}, skipping training.\")\n",
    "else:\n",
    "    logreg_params = {\n",
    "        \"max_iter\": 3000,\n",
    "        \"random_state\": 0,\n",
    "        \"n_jobs\": -1,\n",
    "        \"penalty\": \"l2\",\n",
    "    }\n",
    "\n",
    "    comb_params = logreg_params | best_hp\n",
    "\n",
    "    logreg = LogisticRegression(**comb_params)\n",
    "    logreg.fit(X, y)\n",
    "\n",
    "    joblib.dump(logreg, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "abe9b2fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-evaluation data already exists at data/nf1_model_pre_evaluation_results.parquet, skipping.\n"
     ]
    }
   ],
   "source": [
    "# Path to saved pre-evaluation results\n",
    "data_path = pathlib.Path(\"./data\")\n",
    "data_path.mkdir(parents=True, exist_ok=True)\n",
    "eval_file = data_path / \"nf1_model_pre_evaluation_results.parquet\"\n",
    "\n",
    "if eval_file.exists():\n",
    "    print(f\"Pre-evaluation data already exists at {eval_file}, skipping.\")\n",
    "else:\n",
    "    X_shuf = X.copy()\n",
    "    shuffle_data(X_shuf)\n",
    "\n",
    "    X_test_shuf = X_test.copy()\n",
    "    shuffle_data(X_test_shuf)\n",
    "\n",
    "    store_pre_evaluation_data(X, y, restdf[meta_cols], \"train\")\n",
    "    store_pre_evaluation_data(X_shuf, y, restdf[meta_cols], \"shuffled_train\")\n",
    "\n",
    "    store_pre_evaluation_data(X_test, y_test, testdf[meta_cols], \"test\")\n",
    "    store_pre_evaluation_data(X_test_shuf, y_test, testdf[meta_cols], \"shuffled_test\")\n",
    "\n",
    "    # Store the evaluation data\n",
    "    pd.DataFrame(eval_data).to_parquet(eval_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf1941d",
   "metadata": {},
   "source": [
    "## Extract PR curve metrics and coefficients per feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ff79187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation metric data\n",
    "# The \"metrics\" include precision, recall\n",
    "eval_mets = {\"precision_recall\": defaultdict(list)}\n",
    "\n",
    "\n",
    "def compute_metrics(_df, _plate, _split):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    _df: Pandas Dataframe\n",
    "        Model data to be evaluated.\n",
    "\n",
    "    _plate: String\n",
    "        Name of the plate for storing the metrics\n",
    "\n",
    "    _split: String\n",
    "        Name of the data split for storing the metric\n",
    "    \"\"\"\n",
    "\n",
    "    y_true = _df[gene_column]\n",
    "    y_proba = _df[\"probability_WT\"]\n",
    "\n",
    "    # Store precision and recall data\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_proba, pos_label=\"WT\")\n",
    "    pr_size = precision.shape[0]\n",
    "    eval_mets[\"precision_recall\"][\"precision\"].extend(precision.tolist())\n",
    "    eval_mets[\"precision_recall\"][\"recall\"].extend(recall.tolist())\n",
    "    eval_mets[\"precision_recall\"][\"plate\"].extend([_plate] * pr_size)\n",
    "    eval_mets[\"precision_recall\"][\"datasplit\"].extend([_split] * pr_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d89fc48a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics already exist at pr_results/precision_recall_final_model.parquet, skipping computation.\n"
     ]
    }
   ],
   "source": [
    "# Make directory to save evaluation metrics\n",
    "eval_path = pathlib.Path(\"./pr_results\")\n",
    "eval_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Path to the single metrics file\n",
    "metrics_file = eval_path / \"precision_recall_final_model.parquet\"\n",
    "\n",
    "# If it exists, skip everything and print a message\n",
    "if metrics_file.exists():\n",
    "    print(f\"Metrics already exist at {metrics_file}, skipping computation.\")\n",
    "else:\n",
    "    # Set eval_data as a DataFrame to process\n",
    "    eval_data = pd.DataFrame(eval_data)\n",
    "\n",
    "    # Iterate through each data split\n",
    "    for split in eval_data[\"datasplit\"].unique():\n",
    "\n",
    "        # Calculate metrics for all plates\n",
    "        df_temp = eval_data.loc[(eval_data[\"datasplit\"] == split)].copy()\n",
    "        compute_metrics(df_temp, \"all_plates\", split)\n",
    "\n",
    "        # Calculate metrics for each plate\n",
    "        for plate in eval_data[\"Metadata_Plate\"].unique():\n",
    "            df_temp = eval_data.loc[\n",
    "                (eval_data[\"Metadata_Plate\"] == plate)\n",
    "                & (eval_data[\"datasplit\"] == split)\n",
    "            ].copy()\n",
    "            df_temp = down_sample_by_genotype(df_temp)\n",
    "            compute_metrics(df_temp, plate, split)\n",
    "\n",
    "        # Calculate metrics for each institution *only* from Plate_6 rows\n",
    "        eval_data_Plate_6 = eval_data.loc[eval_data[\"Metadata_Plate\"] == \"Plate_6\"]\n",
    "\n",
    "        for institution in eval_data_Plate_6[\"Metadata_Institution\"].unique():\n",
    "            df_temp = eval_data_Plate_6.loc[\n",
    "                (eval_data_Plate_6[\"Metadata_Institution\"] == institution)\n",
    "                & (eval_data_Plate_6[\"datasplit\"] == split)\n",
    "            ].copy()\n",
    "            df_temp = down_sample_by_genotype(df_temp)\n",
    "\n",
    "            # Rename institution for saving\n",
    "            if institution == \"iNFixion\":\n",
    "                inst_name = \"Plate_6_orig\"\n",
    "            elif institution == \"MGH\":\n",
    "                inst_name = \"Plate_6_deriv\"\n",
    "            else:\n",
    "                inst_name = f\"Plate_6_{institution}\"\n",
    "\n",
    "            compute_metrics(df_temp, inst_name, split)\n",
    "\n",
    "    # Save all computed metrics\n",
    "    for met, met_data in eval_mets.items():\n",
    "        pd.DataFrame(eval_mets[met]).to_parquet(metrics_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "60660057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded trained model from models/best_final_logreg_model.joblib\n",
      "(834, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cytoplasm_RadialDistribution_ZernikePhase_GFP_9_3</td>\n",
       "      <td>0.072248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cytoplasm_RadialDistribution_ZernikeMagnitude_...</td>\n",
       "      <td>-0.140596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cytoplasm_RadialDistribution_ZernikePhase_CY5_7_5</td>\n",
       "      <td>0.001741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nuclei_RadialDistribution_ZernikePhase_DAPI_5_1</td>\n",
       "      <td>0.017364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nuclei_RadialDistribution_ZernikeMagnitude_GFP...</td>\n",
       "      <td>-0.024399</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             feature  coefficient\n",
       "0  Cytoplasm_RadialDistribution_ZernikePhase_GFP_9_3     0.072248\n",
       "1  Cytoplasm_RadialDistribution_ZernikeMagnitude_...    -0.140596\n",
       "2  Cytoplasm_RadialDistribution_ZernikePhase_CY5_7_5     0.001741\n",
       "3    Nuclei_RadialDistribution_ZernikePhase_DAPI_5_1     0.017364\n",
       "4  Nuclei_RadialDistribution_ZernikeMagnitude_GFP...    -0.024399"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model if not already in memory\n",
    "if \"logreg\" not in locals():\n",
    "    model_path = pathlib.Path(\"./models/best_final_logreg_model.joblib\")\n",
    "    logreg = joblib.load(model_path)\n",
    "    print(f\"Loaded trained model from {model_path}\")\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "coeff_dir = pathlib.Path(\"./coeff_results\")\n",
    "coeff_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Create a DataFrame with features and their coefficients\n",
    "coeff_df = pd.DataFrame(\n",
    "    {\"feature\": logreg.feature_names_in_, \"coefficient\": logreg.coef_.reshape(-1)}\n",
    ")\n",
    "\n",
    "# Save to CSV in the coeff_results folder\n",
    "coeff_df.to_csv(coeff_dir / \"final_model_coefficients.csv\", index=False)\n",
    "\n",
    "# Display the first few rows\n",
    "print(coeff_df.shape)\n",
    "coeff_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1f089b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               feature  coefficient\n",
      "509        Nuclei_RadialDistribution_FracAtD_DAPI_4of4     2.181626\n",
      "413         Nuclei_RadialDistribution_FracAtD_RFP_4of4    -1.992575\n",
      "504   Cytoplasm_Intensity_IntegratedIntensityEdge_DAPI    -1.626386\n",
      "519             Cells_Correlation_Correlation_DAPI_GFP     1.534278\n",
      "180      Cytoplasm_RadialDistribution_FracAtD_RFP_4of4    -1.234039\n",
      "..                                                 ...          ...\n",
      "691  Nuclei_RadialDistribution_ZernikeMagnitude_RFP...     0.000883\n",
      "744     Cytoplasm_RadialDistribution_RadialCV_RFP_4of4    -0.000817\n",
      "414    Nuclei_RadialDistribution_ZernikePhase_DAPI_7_1     0.000451\n",
      "707  Cells_RadialDistribution_ZernikeMagnitude_CY5_9_5     0.000323\n",
      "427    Nuclei_RadialDistribution_ZernikePhase_DAPI_9_5     0.000280\n",
      "\n",
      "[834 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Sort coefficients by absolute value, descending\n",
    "sorted_coeff_df = coeff_df.reindex(\n",
    "    coeff_df[\"coefficient\"].abs().sort_values(ascending=False).index\n",
    ")\n",
    "\n",
    "# Print the sorted coefficients\n",
    "print(sorted_coeff_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc3c323",
   "metadata": {},
   "source": [
    "## Load in original model coefficients and outer merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ab8b004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(894, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>coefficient_orig_model</th>\n",
       "      <th>coefficient_new_model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cytoplasm_RadialDistribution_ZernikePhase_GFP_7_3</td>\n",
       "      <td>0.041426</td>\n",
       "      <td>0.013048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nuclei_RadialDistribution_ZernikePhase_CY5_7_5</td>\n",
       "      <td>0.024975</td>\n",
       "      <td>0.050110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nuclei_RadialDistribution_ZernikePhase_RFP_7_7</td>\n",
       "      <td>-0.011419</td>\n",
       "      <td>0.009036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nuclei_RadialDistribution_ZernikeMagnitude_RFP...</td>\n",
       "      <td>-0.053739</td>\n",
       "      <td>-0.027614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cells_RadialDistribution_RadialCV_GFP_1of4</td>\n",
       "      <td>0.019978</td>\n",
       "      <td>0.013382</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             feature  coefficient_orig_model  \\\n",
       "0  Cytoplasm_RadialDistribution_ZernikePhase_GFP_7_3                0.041426   \n",
       "1     Nuclei_RadialDistribution_ZernikePhase_CY5_7_5                0.024975   \n",
       "2     Nuclei_RadialDistribution_ZernikePhase_RFP_7_7               -0.011419   \n",
       "3  Nuclei_RadialDistribution_ZernikeMagnitude_RFP...               -0.053739   \n",
       "4         Cells_RadialDistribution_RadialCV_GFP_1of4                0.019978   \n",
       "\n",
       "   coefficient_new_model  \n",
       "0               0.013048  \n",
       "1               0.050110  \n",
       "2               0.009036  \n",
       "3              -0.027614  \n",
       "4               0.013382  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load in the original model coefficients\n",
    "original_coeff_df = pd.read_parquet(\n",
    "    \"../../2.evaluate_model/model_evaluation_data/feature_importances_qc.parquet\"\n",
    ")\n",
    "\n",
    "# Rename columns to 'feature' and 'coefficient'\n",
    "original_coeff_df = original_coeff_df.rename(\n",
    "    columns={\n",
    "        original_coeff_df.columns[0]: \"feature\",\n",
    "        original_coeff_df.columns[1]: \"coefficient\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# Perform an outer merge of the original model and the new model\n",
    "merged_coefs = pd.merge(\n",
    "    original_coeff_df,\n",
    "    coeff_df,\n",
    "    on=\"feature\",\n",
    "    how=\"outer\",\n",
    "    suffixes=(\"_orig_model\", \"_new_model\"),\n",
    ")\n",
    "\n",
    "# Fill NaN values with 0\n",
    "merged_coefs.fillna(0, inplace=True)\n",
    "\n",
    "# Save the merged coefficients to a CSV file\n",
    "merged_coefs.to_csv(\n",
    "    coeff_dir / \"merged_coefficients_original_new_model.csv\", index=False\n",
    ")\n",
    "\n",
    "# Display the merged dataframe\n",
    "print(merged_coefs.shape)\n",
    "merged_coefs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cd8f5e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman correlation: 0.7433, p-value: 6.3344e-158\n"
     ]
    }
   ],
   "source": [
    "spearman_corr, p_value = spearmanr(\n",
    "    merged_coefs[\"coefficient_orig_model\"], merged_coefs[\"coefficient_new_model\"]\n",
    ")\n",
    "\n",
    "print(f\"Spearman correlation: {spearman_corr:.4f}, p-value: {p_value:.4e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nf1_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
