{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jukit_cell_id": "quD0wavRs0"
   },
   "source": [
    "# Classify WT and Null Genotypes Logistic Regression\n",
    "Plates 3, 3p, and 5 are used in all splits to classify genotypes either (WT or Null)\n",
    "The feature selected data is used in all data splits.\n",
    "Pre-evaluation metrics are stored from all splits and these plates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-19T20:21:08.300458Z",
     "iopub.status.busy": "2024-07-19T20:21:08.300354Z",
     "iopub.status.idle": "2024-07-19T20:21:08.745708Z",
     "shell.execute_reply": "2024-07-19T20:21:08.745264Z"
    },
    "jukit_cell_id": "RcnXoNLyM2"
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import random\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import dump\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import parallel_backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-06T10:32:18 [train_12_08_07_53_logreg_train] INFO: Initialized logger.\n"
     ]
    }
   ],
   "source": [
    "MODEL_ID = \"logreg\"\n",
    "ROLE = \"train\"\n",
    "\n",
    "# ---- Standard logging setup for this project ----\n",
    "\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import pathlib\n",
    "\n",
    "# ============================================\n",
    "# 1) Choose a RUN_ID\n",
    "# ============================================\n",
    "RUN_ID = datetime.now().strftime(\"%m_%d_%H_%M\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "RUN_ID = \"12_08_07_53\"\n",
    "ANALYSIS_TYPE = \"train\"\n",
    "\n",
    "\n",
    "def setup_logger(\n",
    "    run_id: str,\n",
    "    model_id: str,\n",
    "    role: str,\n",
    "    log_dir: str = \"logs\",\n",
    "    analysis_type: str = ANALYSIS_TYPE,\n",
    ") -> logging.Logger:\n",
    "    \"\"\"\n",
    "    Create a logger that writes to both stdout and a log file.\n",
    "\n",
    "    - Logger name:  \"<analysis_type>_<run_id>_<model_id>_<role>\"\n",
    "    - Log file:     \"log_<analysis_type>_<run_id>_<model_id>.log\" in `log_dir`\n",
    "      (shared by all notebooks for the same model & run & analysis_type).\n",
    "    \"\"\"\n",
    "    log_path = pathlib.Path(log_dir)\n",
    "    log_path.mkdir(exist_ok=True)\n",
    "\n",
    "    logger_name = f\"{analysis_type}_{run_id}_{model_id}_{role}\"\n",
    "    logger = logging.getLogger(logger_name)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    logger.propagate = False  # don't duplicate logs to root logger\n",
    "\n",
    "    # Avoid adding handlers multiple times if the cell is re-run\n",
    "    if not logger.handlers:\n",
    "        # Common formatter for both handlers\n",
    "        formatter = logging.Formatter(\n",
    "            fmt=\"%(asctime)s [%(name)s] %(levelname)s: %(message)s\",\n",
    "            datefmt=\"%Y-%m-%dT%H:%M:%S\",\n",
    "        )\n",
    "\n",
    "        # Stream handler (stdout)\n",
    "        stream_handler = logging.StreamHandler()\n",
    "        stream_handler.setLevel(logging.INFO)\n",
    "        stream_handler.setFormatter(formatter)\n",
    "        logger.addHandler(stream_handler)\n",
    "\n",
    "        # File handler (one file per analysis_type + run_id + model_id)\n",
    "        log_file = log_path / f\"log_{analysis_type}_{run_id}_{model_id}.log\"\n",
    "        file_handler = logging.FileHandler(log_file)\n",
    "        file_handler.setLevel(logging.INFO)\n",
    "        file_handler.setFormatter(formatter)\n",
    "        logger.addHandler(file_handler)\n",
    "\n",
    "    return logger\n",
    "\n",
    "\n",
    "\n",
    "logger = setup_logger(RUN_ID, MODEL_ID, ROLE)\n",
    "logger.info(\"Initialized logger.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jukit_cell_id": "tdSJIClZGb"
   },
   "source": [
    "## Find the root of the git repo on the host system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-19T20:21:08.747608Z",
     "iopub.status.busy": "2024-07-19T20:21:08.747418Z",
     "iopub.status.idle": "2024-07-19T20:21:08.750332Z",
     "shell.execute_reply": "2024-07-19T20:21:08.750003Z"
    },
    "jukit_cell_id": "QmTyYX7yVG"
   },
   "outputs": [],
   "source": [
    "# Get the current working directory\n",
    "cwd = pathlib.Path.cwd()\n",
    "\n",
    "if (cwd / \".git\").is_dir():\n",
    "    root_dir = cwd\n",
    "\n",
    "else:\n",
    "    root_dir = None\n",
    "    for parent in cwd.parents:\n",
    "        if (parent / \".git\").is_dir():\n",
    "            root_dir = parent\n",
    "            break\n",
    "\n",
    "# Check if a Git root directory was found\n",
    "if root_dir is None:\n",
    "    raise FileNotFoundError(\"No Git root directory found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jukit_cell_id": "eSQXraMRCI"
   },
   "source": [
    "## Define paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jukit_cell_id": "cZx8xNCyZq"
   },
   "source": [
    "### Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-19T20:21:08.751806Z",
     "iopub.status.busy": "2024-07-19T20:21:08.751585Z",
     "iopub.status.idle": "2024-07-19T20:21:09.067096Z",
     "shell.execute_reply": "2024-07-19T20:21:09.066546Z"
    },
    "jukit_cell_id": "xdgUvt0md2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-06T10:32:19 [train_12_08_07_53_logreg_train] INFO: Number of single-cells total per plate:\n",
      "2025-12-06T10:32:19 [train_12_08_07_53_logreg_train] INFO: Plate 3: 10206\n",
      "2025-12-06T10:32:19 [train_12_08_07_53_logreg_train] INFO: Plate 3 prime: 5126\n",
      "2025-12-06T10:32:19 [train_12_08_07_53_logreg_train] INFO: Plate 5: 5348\n"
     ]
    }
   ],
   "source": [
    "# OPTIONAL: If the data (within the cell painting directory) is stored in a different location, add location here\n",
    "repo_dir = pathlib.Path(\n",
    "    \"/Users/marktalbot/Documents/VC Studio Homework Folders/HighRisk/nf1_schwann_cell_painting_data\"\n",
    ")\n",
    "\n",
    "# Set data level\n",
    "data_level = \"cleaned\"\n",
    "\n",
    "# Main directory path (converted or cleaned data)\n",
    "if data_level == \"cleaned\":\n",
    "    data_dir = pathlib.Path(\n",
    "        repo_dir / \"3.processing_features/data/single_cell_profiles/cleaned_sc_profiles\"\n",
    "    )\n",
    "else:\n",
    "    data_dir = pathlib.Path(\n",
    "        repo_dir / \"3.processing_features/data/single_cell_profiles\"\n",
    "    )\n",
    "\n",
    "plate3df_path = pathlib.Path(data_dir / \"Plate_3_sc_feature_selected.parquet\").resolve(\n",
    "    strict=True\n",
    ")\n",
    "plate3pdf_path = pathlib.Path(\n",
    "    data_dir / \"Plate_3_prime_sc_feature_selected.parquet\"\n",
    ").resolve(strict=True)\n",
    "plate5df_path = pathlib.Path(data_dir / \"Plate_5_sc_feature_selected.parquet\").resolve(\n",
    "    strict=True\n",
    ")\n",
    "\n",
    "plate3df = pd.read_parquet(plate3df_path)\n",
    "plate3pdf = pd.read_parquet(plate3pdf_path)\n",
    "plate5df = pd.read_parquet(plate5df_path)\n",
    "\n",
    "logger.info(\"Number of single-cells total per plate:\")\n",
    "logger.info(f\"Plate 3: {plate3df.shape[0]}\")\n",
    "logger.info(f\"Plate 3 prime: {plate3pdf.shape[0]}\")\n",
    "logger.info(f\"Plate 5: {plate5df.shape[0]}\")\n",
    "\n",
    "# Set the seed\n",
    "rng = np.random.default_rng(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jukit_cell_id": "G0IEMyaFva"
   },
   "source": [
    "### Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-19T20:21:09.069370Z",
     "iopub.status.busy": "2024-07-19T20:21:09.069143Z",
     "iopub.status.idle": "2024-07-19T20:21:09.071752Z",
     "shell.execute_reply": "2024-07-19T20:21:09.071419Z"
    },
    "jukit_cell_id": "byqT0qeQSc"
   },
   "outputs": [],
   "source": [
    "data_path = pathlib.Path(\"data\")\n",
    "data_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jukit_cell_id": "YtBIYKchGc"
   },
   "source": [
    "## Splitting and Processing\n",
    "Functions to split and process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-19T20:21:09.073333Z",
     "iopub.status.busy": "2024-07-19T20:21:09.073098Z",
     "iopub.status.idle": "2024-07-19T20:21:09.077658Z",
     "shell.execute_reply": "2024-07-19T20:21:09.077314Z"
    },
    "jukit_cell_id": "qP1KgMqkE7"
   },
   "outputs": [],
   "source": [
    "gene_column = \"Metadata_genotype\"\n",
    "\n",
    "\n",
    "def down_sample_by_genotype(_df):\n",
    "    \"\"\"\n",
    "    Return an equal number of cells from each genotype.\n",
    "    The number of cells in a genotype is the minimum number of cells from all genotypes.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    _df: Pandas Dataframe\n",
    "        The data to be downsampled by the gene_column column.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    The dataframe down-sampled by genotype.\n",
    "    \"\"\"\n",
    "\n",
    "    min_gene = _df[gene_column].value_counts().min()\n",
    "    return _df.groupby(gene_column, group_keys=False).apply(\n",
    "        lambda x: x.sample(n=min_gene, random_state=0)\n",
    "    )\n",
    "\n",
    "\n",
    "def process_plates(_df):\n",
    "    \"\"\"\n",
    "    Drop rows with nans from the single cell data and remove HET cells.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    _df: Pandas Dataframe\n",
    "        Uncleaned plate data with nans and HET cells to be removed. Contains the column \"Metadata_genotype\".\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    _df: Pandas Dataframe\n",
    "        Cleaned single cell data by removing nans and HET cells.\n",
    "    \"\"\"\n",
    "\n",
    "    _df.dropna(inplace=True)\n",
    "    _df = _df.loc[_df[gene_column] != \"HET\"]\n",
    "    return _df\n",
    "\n",
    "\n",
    "def shuffle_data(_X):\n",
    "    \"\"\"\n",
    "    Shuffle the columns of the input dataframe independently.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    _X: Pandas Dataframe\n",
    "        Input feature data for shuffling the columns.\n",
    "    \"\"\"\n",
    "\n",
    "    for column in _X.columns:\n",
    "        _X[column] = rng.permutation(_X[column])\n",
    "\n",
    "\n",
    "def store_pre_evaluation_data(_X, _y, _metadata, _datasplit):\n",
    "    \"\"\"\n",
    "    Store model data to evaluate performance.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    _X: Pandas Dataframe\n",
    "        Feature dataframe from a given plate and data split.\n",
    "\n",
    "    _y: Numpy Array\n",
    "        A numerically-encoded label vector ordered according to _X.\n",
    "\n",
    "    _metadata: Pandas Dataframe\n",
    "        Plate name.\n",
    "\n",
    "    _datasplit: String\n",
    "        Data split name.\n",
    "    \"\"\"\n",
    "\n",
    "    eval_data[f\"probability_{probability_class}\"].extend(\n",
    "        logreg.predict_proba(_X)[:, 1].tolist()\n",
    "    )\n",
    "    eval_data[\"datasplit\"].extend([_datasplit] * _X.shape[0])\n",
    "    eval_data[\"predicted_genotype\"].extend(logreg.predict(_X).tolist())\n",
    "    eval_data[\"true_genotype\"].extend(_y.tolist())\n",
    "    for meta_col in _metadata.columns:\n",
    "        eval_data[meta_col].extend(_metadata[meta_col].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jukit_cell_id": "R7X8nR4SsB"
   },
   "source": [
    "## Split and process plates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-19T20:21:09.079175Z",
     "iopub.status.busy": "2024-07-19T20:21:09.078970Z",
     "iopub.status.idle": "2024-07-19T20:21:09.081229Z",
     "shell.execute_reply": "2024-07-19T20:21:09.080896Z"
    },
    "jukit_cell_id": "z9zzUXpQ67"
   },
   "outputs": [],
   "source": [
    "def create_splits(_wells, _plate):\n",
    "    \"\"\"\n",
    "    Create data splits for model training. The splits are rest (train and validation) and test.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    _wells: List(String)\n",
    "        The well names from which single cells will be used in the test set.\n",
    "\n",
    "    _plate: Pandas Dataframe\n",
    "        Single cell data from one of the plate's containing a \"Metadata_Well\" column.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Dataframes of the split single cell data.\n",
    "    \"\"\"\n",
    "\n",
    "    return (\n",
    "        _plate[~_plate[\"Metadata_Well\"].isin(_wells)],\n",
    "        _plate[_plate[\"Metadata_Well\"].isin(_wells)],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-19T20:21:09.082677Z",
     "iopub.status.busy": "2024-07-19T20:21:09.082489Z",
     "iopub.status.idle": "2024-07-19T20:21:09.372731Z",
     "shell.execute_reply": "2024-07-19T20:21:09.372244Z"
    },
    "jukit_cell_id": "qrteU1j4F9"
   },
   "outputs": [],
   "source": [
    "plate3df = process_plates(plate3df)\n",
    "p3_wells = [\"C11\", \"E11\", \"C3\", \"F3\"]\n",
    "rest3df, test3df = create_splits(p3_wells, plate3df)\n",
    "rest3df, test3df = down_sample_by_genotype(rest3df), down_sample_by_genotype(test3df)\n",
    "\n",
    "plate3pdf = process_plates(plate3pdf)\n",
    "p3p_wells = [\"F11\", \"G11\", \"C3\", \"F3\"]\n",
    "rest3pdf, test3pdf = create_splits(p3p_wells, plate3pdf)\n",
    "rest3pdf, test3pdf = down_sample_by_genotype(rest3pdf), down_sample_by_genotype(\n",
    "    test3pdf\n",
    ")\n",
    "\n",
    "plate5df = process_plates(plate5df)\n",
    "p5_wells = [\"C9\", \"E11\", \"E3\", \"G3\"]\n",
    "rest5df, test5df = create_splits(p5_wells, plate5df)\n",
    "rest5df, test5df = down_sample_by_genotype(rest5df), down_sample_by_genotype(test5df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jukit_cell_id": "pnZI3c8SRh"
   },
   "source": [
    "## Combine plate columns across each data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-19T20:21:09.374908Z",
     "iopub.status.busy": "2024-07-19T20:21:09.374782Z",
     "iopub.status.idle": "2024-07-19T20:21:09.537487Z",
     "shell.execute_reply": "2024-07-19T20:21:09.536874Z"
    },
    "jukit_cell_id": "JPOu8mYg6w"
   },
   "outputs": [],
   "source": [
    "# Columns common to all plates\n",
    "plate_cols = list(\n",
    "    set(plate5df.columns) & set(plate3df.columns) & set(plate3pdf.columns)\n",
    ")\n",
    "\n",
    "restdf = pd.concat(\n",
    "    [rest3df[plate_cols], rest3pdf[plate_cols], rest5df[plate_cols]], ignore_index=True\n",
    ").reset_index(drop=True)\n",
    "\n",
    "testdf = pd.concat(\n",
    "    [test3df[plate_cols], test3pdf[plate_cols], test5df[plate_cols]], ignore_index=True\n",
    ").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jukit_cell_id": "7ckV1bxaO2"
   },
   "source": [
    "## Encode genotypes and extract feature data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-19T20:21:09.539765Z",
     "iopub.status.busy": "2024-07-19T20:21:09.539606Z",
     "iopub.status.idle": "2024-07-19T20:21:09.543873Z",
     "shell.execute_reply": "2024-07-19T20:21:09.543515Z"
    },
    "jukit_cell_id": "NTCaf4xXY1"
   },
   "outputs": [],
   "source": [
    "meta_cols = testdf.filter(like=\"Metadata\").columns\n",
    "feat_cols = testdf.drop(columns=meta_cols).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-19T20:21:09.545393Z",
     "iopub.status.busy": "2024-07-19T20:21:09.545209Z",
     "iopub.status.idle": "2024-07-19T20:21:09.566594Z",
     "shell.execute_reply": "2024-07-19T20:21:09.566165Z"
    },
    "jukit_cell_id": "m7apYZPkME"
   },
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "\n",
    "y = le.fit_transform(restdf[\"Metadata_genotype\"])\n",
    "X = restdf.drop(columns=meta_cols)\n",
    "\n",
    "y_test = le.fit_transform(testdf[\"Metadata_genotype\"])\n",
    "X_test = testdf.drop(columns=meta_cols)\n",
    "\n",
    "# Class for saving probabilities\n",
    "probability_class = le.inverse_transform([1])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jukit_cell_id": "RN1oKgi6ph"
   },
   "source": [
    "# Train Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jukit_cell_id": "U6eyLl0ZPH"
   },
   "source": [
    "## Specify parameters for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-19T20:21:09.568474Z",
     "iopub.status.busy": "2024-07-19T20:21:09.568257Z",
     "iopub.status.idle": "2024-07-19T20:21:09.571406Z",
     "shell.execute_reply": "2024-07-19T20:21:09.571078Z"
    },
    "jukit_cell_id": "sZFW0vn0zv"
   },
   "outputs": [],
   "source": [
    "logreg_params = {\n",
    "    \"max_iter\": 250,\n",
    "    \"random_state\": 0,\n",
    "    \"n_jobs\": -1,\n",
    "    \"penalty\": \"l2\",\n",
    "}\n",
    "\n",
    "# Random sampling range of hyperparameter\n",
    "param_ranges = {\n",
    "    \"C\": (0, 200)\n",
    "}\n",
    "\n",
    "# Number of iteration to optimize hyperparameters\n",
    "rand_iter = 50\n",
    "\n",
    "# Best accuracy\n",
    "best_acc = 0\n",
    "\n",
    "# Initial accuracy\n",
    "acc = 0\n",
    "\n",
    "# Number of folds\n",
    "n_splits = 8\n",
    "\n",
    "# Generate hyperparameter samples\n",
    "random_params = {\n",
    "    i:\n",
    "    {key: random.uniform(*param_ranges[key]) for key in param_ranges}\n",
    "    for i in range(rand_iter)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jukit_cell_id": "uBQzEOgaBh"
   },
   "source": [
    "## Hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-19T20:21:09.572973Z",
     "iopub.status.busy": "2024-07-19T20:21:09.572786Z",
     "iopub.status.idle": "2024-07-20T03:13:23.267614Z",
     "shell.execute_reply": "2024-07-20T03:13:23.267154Z"
    },
    "jukit_cell_id": "rHYvmbnZNf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-06T10:32:19 [train_12_08_07_53_logreg_train] INFO: [INFO] Iteration 1 of 50: trying C = 132.670\n",
      "/opt/miniconda3/envs/nf1_analysis/lib/python3.9/site-packages/sklearn/utils/deprecation.py:71: FutureWarning: Class parallel_backend is deprecated; deprecated in 1.5 to be removed in 1.7. Use joblib.{} instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/opt/miniconda3/envs/nf1_analysis/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/miniconda3/envs/nf1_analysis/lib/python3.9/site-packages/sklearn/utils/deprecation.py:71: FutureWarning: Class parallel_backend is deprecated; deprecated in 1.5 to be removed in 1.7. Use joblib.{} instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "Process SpawnPoolWorker-12:\n",
      "Process SpawnPoolWorker-15:\n",
      "Process SpawnPoolWorker-10:\n",
      "Process SpawnPoolWorker-14:\n",
      "Process SpawnPoolWorker-11:\n",
      "Process SpawnPoolWorker-13:\n",
      "Process SpawnPoolWorker-16:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/miniconda3/envs/nf1_analysis/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/miniconda3/envs/nf1_analysis/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/miniconda3/envs/nf1_analysis/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/miniconda3/envs/nf1_analysis/lib/python3.9/site-packages/joblib/pool.py\", line 142, in get\n",
      "    racquire()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/miniconda3/envs/nf1_analysis/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/miniconda3/envs/nf1_analysis/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/miniconda3/envs/nf1_analysis/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/miniconda3/envs/nf1_analysis/lib/python3.9/site-packages/joblib/pool.py\", line 142, in get\n",
      "    racquire()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/miniconda3/envs/nf1_analysis/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/miniconda3/envs/nf1_analysis/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/miniconda3/envs/nf1_analysis/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/miniconda3/envs/nf1_analysis/lib/python3.9/site-packages/joblib/pool.py\", line 142, in get\n",
      "    racquire()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/miniconda3/envs/nf1_analysis/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/miniconda3/envs/nf1_analysis/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/miniconda3/envs/nf1_analysis/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/miniconda3/envs/nf1_analysis/lib/python3.9/site-packages/joblib/pool.py\", line 142, in get\n",
      "    racquire()\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/miniconda3/envs/nf1_analysis/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/miniconda3/envs/nf1_analysis/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/miniconda3/envs/nf1_analysis/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/miniconda3/envs/nf1_analysis/lib/python3.9/site-packages/joblib/pool.py\", line 144, in get\n",
      "    return recv()\n",
      "  File \"/opt/miniconda3/envs/nf1_analysis/lib/python3.9/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/opt/miniconda3/envs/nf1_analysis/lib/python3.9/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/opt/miniconda3/envs/nf1_analysis/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/miniconda3/envs/nf1_analysis/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/miniconda3/envs/nf1_analysis/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/miniconda3/envs/nf1_analysis/lib/python3.9/site-packages/joblib/pool.py\", line 142, in get\n",
      "    racquire()\n",
      "  File \"/opt/miniconda3/envs/nf1_analysis/lib/python3.9/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/miniconda3/envs/nf1_analysis/lib/python3.9/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/miniconda3/envs/nf1_analysis/lib/python3.9/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/miniconda3/envs/nf1_analysis/lib/python3.9/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/miniconda3/envs/nf1_analysis/lib/python3.9/site-packages/joblib/pool.py\", line 142, in get\n",
      "    racquire()\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 28\u001b[0m\n\u001b[1;32m     24\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\n\u001b[1;32m     25\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m, category\u001b[38;5;241m=\u001b[39mConvergenceWarning, module\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msklearn\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     26\u001b[0m         )\n\u001b[1;32m     27\u001b[0m         logreg \u001b[38;5;241m=\u001b[39m LogisticRegression(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcomb_params)\n\u001b[0;32m---> 28\u001b[0m         \u001b[43mlogreg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Cumulative accuracy for all folds\u001b[39;00m\n\u001b[1;32m     31\u001b[0m preds \u001b[38;5;241m=\u001b[39m logreg\u001b[38;5;241m.\u001b[39mpredict(X_val)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/nf1_analysis/lib/python3.9/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/nf1_analysis/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1350\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1347\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1348\u001b[0m     n_threads \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1350\u001b[0m fold_coefs_ \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1352\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1353\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1354\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpos_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1355\u001b[0m \u001b[43m        \u001b[49m\u001b[43mCs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mC_\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1356\u001b[0m \u001b[43m        \u001b[49m\u001b[43ml1_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ml1_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1357\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfit_intercept\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_intercept\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1358\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1359\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1360\u001b[0m \u001b[43m        \u001b[49m\u001b[43msolver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msolver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1361\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmulti_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmulti_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1362\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1363\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1364\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1365\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1366\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcoef\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwarm_start_coef_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1367\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpenalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpenalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1368\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_squared_sum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_squared_sum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1369\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1370\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1371\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1372\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclass_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarm_start_coef_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mclasses_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarm_start_coef\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1373\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1375\u001b[0m fold_coefs_, _, n_iter_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mfold_coefs_)\n\u001b[1;32m   1376\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(n_iter_, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint32)[:, \u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/opt/miniconda3/envs/nf1_analysis/lib/python3.9/site-packages/sklearn/utils/parallel.py:77\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     72\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     73\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     74\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     76\u001b[0m )\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/nf1_analysis/lib/python3.9/site-packages/joblib/parallel.py:2072\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2066\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2067\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2068\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2069\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2070\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2072\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/nf1_analysis/lib/python3.9/site-packages/joblib/parallel.py:1682\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1679\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1681\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1682\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1684\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1685\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1686\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1687\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1688\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/nf1_analysis/lib/python3.9/site-packages/joblib/parallel.py:1800\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1789\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_ordered:\n\u001b[1;32m   1790\u001b[0m     \u001b[38;5;66;03m# Case ordered: wait for completion (or error) of the next job\u001b[39;00m\n\u001b[1;32m   1791\u001b[0m     \u001b[38;5;66;03m# that have been dispatched and not retrieved yet. If no job\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1795\u001b[0m     \u001b[38;5;66;03m# control only have to be done on the amount of time the next\u001b[39;00m\n\u001b[1;32m   1796\u001b[0m     \u001b[38;5;66;03m# dispatched job is pending.\u001b[39;00m\n\u001b[1;32m   1797\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (nb_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   1798\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING\n\u001b[1;32m   1799\u001b[0m     ):\n\u001b[0;32m-> 1800\u001b[0m         \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1801\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1803\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m nb_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1804\u001b[0m     \u001b[38;5;66;03m# Case unordered: jobs are added to the list of jobs to\u001b[39;00m\n\u001b[1;32m   1805\u001b[0m     \u001b[38;5;66;03m# retrieve `self._jobs` only once completed or in error, which\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1811\u001b[0m     \u001b[38;5;66;03m# timeouts before any other dispatched job has completed and\u001b[39;00m\n\u001b[1;32m   1812\u001b[0m     \u001b[38;5;66;03m# been added to `self._jobs` to be retrieved.\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Store model results for evaluation\n",
    "eval_data = defaultdict(list)\n",
    "\n",
    "# Iterate through hyperparameters\n",
    "for idx, rparams in random_params.items():\n",
    "    logger.info(f\"[INFO] Iteration {idx + 1} of {len(random_params)}: trying C = {rparams.get('C', 'N/A'):.3f}\")\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=0)\n",
    "\n",
    "    # Combine parameters in current search with logistic regression parameters\n",
    "    comb_params = logreg_params | rparams\n",
    "\n",
    "    # Loop through the folds\n",
    "    for fold, (train_index, val_index) in enumerate(skf.split(X, y)):\n",
    "\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "        X_val_shuf = X_val.copy()\n",
    "        shuffle_data(X_val_shuf)\n",
    "\n",
    "        # Prevent the convergence warning in sklearn\n",
    "        with parallel_backend(\"multiprocessing\"):\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.filterwarnings(\n",
    "                    \"ignore\", category=ConvergenceWarning, module=\"sklearn\"\n",
    "                )\n",
    "                logreg = LogisticRegression(**comb_params)\n",
    "                logreg.fit(X_train, y_train)\n",
    "\n",
    "        # Cumulative accuracy for all folds\n",
    "        preds = logreg.predict(X_val)\n",
    "        preds_shuf = logreg.predict(X_val_shuf)\n",
    "        acc += accuracy_score(y_val, preds)\n",
    "\n",
    "        store_pre_evaluation_data(X_val, y_val, restdf.iloc[val_index][meta_cols], \"val\")\n",
    "        store_pre_evaluation_data(X_val_shuf, y_val, restdf.iloc[val_index][meta_cols], \"shuffled_val\")\n",
    "\n",
    "    # Average accuracy for the folds\n",
    "    acc = acc / n_splits\n",
    "\n",
    "    # Store the data with the best performance\n",
    "    if acc > best_acc:\n",
    "        best_hparam = eval_data.copy()\n",
    "        best_acc = acc\n",
    "        best_hp = rparams\n",
    "\n",
    "logger.info(f\"Best average validation accuracy = {best_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jukit_cell_id": "4D42C7hQua"
   },
   "source": [
    "## Retrain model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T03:13:23.297509Z",
     "iopub.status.busy": "2024-07-20T03:13:23.297314Z",
     "iopub.status.idle": "2024-07-20T03:13:32.871574Z",
     "shell.execute_reply": "2024-07-20T03:13:32.870797Z"
    },
    "jukit_cell_id": "FNeiZcYJzz"
   },
   "outputs": [],
   "source": [
    "logreg_params = {\n",
    "    \"max_iter\": 3000,\n",
    "    \"random_state\": 0,\n",
    "    \"n_jobs\": -1,\n",
    "    \"penalty\": \"l2\",\n",
    "}\n",
    "\n",
    "comb_params = logreg_params | best_hp\n",
    "\n",
    "logreg = LogisticRegression(**comb_params)\n",
    "logreg.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jukit_cell_id": "RjJUksr4Hk"
   },
   "source": [
    "## Shuffle train and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T03:13:32.875142Z",
     "iopub.status.busy": "2024-07-20T03:13:32.874899Z",
     "iopub.status.idle": "2024-07-20T03:13:39.567201Z",
     "shell.execute_reply": "2024-07-20T03:13:39.566773Z"
    },
    "jukit_cell_id": "Hwc1Sal9AZ"
   },
   "outputs": [],
   "source": [
    "X_shuf = X.copy()\n",
    "shuffle_data(X_shuf)\n",
    "\n",
    "X_test_shuf = X_test.copy()\n",
    "shuffle_data(X_test_shuf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jukit_cell_id": "bhXkY9LWcL"
   },
   "source": [
    "# Save models and model data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jukit_cell_id": "IKb986kQIQ"
   },
   "source": [
    "## Store pre-evaluation split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T03:13:39.569719Z",
     "iopub.status.busy": "2024-07-20T03:13:39.569566Z",
     "iopub.status.idle": "2024-07-20T03:13:39.726629Z",
     "shell.execute_reply": "2024-07-20T03:13:39.726190Z"
    },
    "jukit_cell_id": "4uw0FlkRQV"
   },
   "outputs": [],
   "source": [
    "store_pre_evaluation_data(X, y, restdf[meta_cols], \"train\")\n",
    "store_pre_evaluation_data(X_shuf, y, restdf[meta_cols], \"shuffled_train\")\n",
    "\n",
    "store_pre_evaluation_data(X_test, y_test, testdf[meta_cols], \"test\")\n",
    "store_pre_evaluation_data(X_test_shuf, y_test, testdf[meta_cols], \"shuffled_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T03:13:39.729183Z",
     "iopub.status.busy": "2024-07-20T03:13:39.729027Z",
     "iopub.status.idle": "2024-07-20T03:14:13.289064Z",
     "shell.execute_reply": "2024-07-20T03:14:13.288371Z"
    },
    "jukit_cell_id": "ONBKrPrWx0"
   },
   "outputs": [],
   "source": [
    "suffix = \"_qc\" if data_level == \"cleaned\" else \"\"\n",
    "\n",
    "dump(logreg, f\"{data_path}/trained_nf1_model{suffix}.joblib\")\n",
    "dump(le, f\"{data_path}/trained_nf1_model_label_encoder{suffix}.joblib\")\n",
    "pd.DataFrame(eval_data).to_parquet(f\"{data_path}/nf1_model_pre_evaluation_results{suffix}.parquet\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "nf1_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
